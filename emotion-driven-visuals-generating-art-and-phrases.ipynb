{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n!pip install -q torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n!pip install -q diffusers transformers accelerate xformers==0.0.20 gradio plotly\n!pip install -q matplotlib seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:51:27.141573Z","iopub.execute_input":"2024-11-20T20:51:27.142279Z","iopub.status.idle":"2024-11-20T20:54:00.078332Z","shell.execute_reply.started":"2024-11-20T20:51:27.142235Z","shell.execute_reply":"2024-11-20T20:54:00.076849Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nFound existing installation: torchvision 0.19.0\nUninstalling torchvision-0.19.0:\n  Successfully uninstalled torchvision-0.19.0\nFound existing installation: torchaudio 2.4.0\nUninstalling torchaudio-2.4.0:\n  Successfully uninstalled torchaudio-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Imports and Environment Setup**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom transformers import pipeline\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nimport nltk\nimport gradio as gr\n\n# Download NLTK resources\nnltk.download('punkt', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True)\nnltk.download('maxent_ne_chunker', quiet=True)\nnltk.download('words', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:00.081056Z","iopub.execute_input":"2024-11-20T20:54:00.081484Z","iopub.status.idle":"2024-11-20T20:54:21.289429Z","shell.execute_reply.started":"2024-11-20T20:54:00.081438Z","shell.execute_reply":"2024-11-20T20:54:21.288630Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"**Environment and Model Initialization**","metadata":{}},{"cell_type":"code","source":"def setup_environment():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}, CUDA Available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    return device\n\ndef initialize_models(device):\n    try:\n        # Initializes sentiment analyzer, topic classifier, and image generation model\n        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")\n        topic_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n        \n        model_id = \"Lykon/dreamshaper-8\"\n        model = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if str(device) == 'cuda' else torch.float32,\n            safety_checker=None\n        )\n        model.scheduler = DPMSolverMultistepScheduler.from_config(\n            model.scheduler.config,\n            algorithm_type=\"dpmsolver++\",\n            solver_order=2\n        )\n\n        if str(device) == 'cuda':\n            model.enable_xformers_memory_efficient_attention()\n            model.enable_attention_slicing()\n            \n        model = model.to(device)\n        print(\"Models Initialized Successfully\")\n        return sentiment_analyzer, topic_classifier, model\n    except Exception as e:\n        print(f\"Error Initializing Models: {e}\")\n        return None, None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.290527Z","iopub.execute_input":"2024-11-20T20:54:21.291398Z","iopub.status.idle":"2024-11-20T20:54:21.298186Z","shell.execute_reply.started":"2024-11-20T20:54:21.291369Z","shell.execute_reply":"2024-11-20T20:54:21.297453Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Visualization Functions**","metadata":{}},{"cell_type":"code","source":"def create_sentiment_pie_chart(sentiment_score, sentiment_label):\n    \"\"\"Create a pie chart visualizing sentiment\"\"\"\n    labels = ['Positive', 'Negative'] if sentiment_label == 'POSITIVE' else ['Negative', 'Positive']\n    values = [sentiment_score, 1 - sentiment_score]\n    \n    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n    fig.update_traces(\n        hoverinfo='label+percent', \n        textinfo='percent', \n        textfont_size=15,\n        marker=dict(colors=['green', 'red'] if sentiment_label == 'POSITIVE' else ['red', 'green'])\n    )\n    return fig\n\ndef create_topic_bar_chart(topic_scores, topic_labels):\n    \"\"\"Create a bar chart visualizing topic classification\"\"\"\n    fig = px.bar(\n        x=topic_labels, \n        y=topic_scores, \n        labels={'x':'Topics', 'y':'Confidence Score'},\n        title='Topic Classification Confidence'\n    )\n    return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.300537Z","iopub.execute_input":"2024-11-20T20:54:21.300898Z","iopub.status.idle":"2024-11-20T20:54:21.322056Z","shell.execute_reply.started":"2024-11-20T20:54:21.300862Z","shell.execute_reply":"2024-11-20T20:54:21.321239Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Prompt Analysis Function**","metadata":{}},{"cell_type":"code","source":"def analyze_prompt(prompt, sentiment_analyzer, topic_classifier):\n    # Analyze sentiment of the prompt\n    sentiment_result = sentiment_analyzer(prompt)[0]\n    \n    # Classify potential topics\n    candidate_topics = [\n        \"landscape\", \"portrait\", \"abstract\", \"nature\", \"urban\",\n        \"fantasy\", \"sci-fi\", \"realistic\", \"artistic\", \"architectural\"\n    ]\n    topic_result = topic_classifier(prompt, candidate_topics)\n\n    # Determine image style based on keywords\n    style_keywords = {\n        'realistic': ['realistic', 'photographic', 'natural', 'real'],\n        'artistic': ['artistic', 'painted', 'stylized', 'abstract'],\n        'fantasy': ['magical', 'fantasy', 'mythical', 'mystical'],\n        'sci-fi': ['futuristic', 'sci-fi', 'technological', 'cyber']\n    }\n\n    # Calculate style scores based on keyword matches\n    style_scores = {style: sum(1 for keyword in keywords if keyword in prompt.lower())\n                    for style, keywords in style_keywords.items()}\n    primary_style = max(style_scores.items(), key=lambda x: x[1])[0] if any(style_scores.values()) else 'realistic'\n\n    return {\n        'sentiment': sentiment_result['label'],\n        'sentiment_score': sentiment_result['score'],\n        'primary_topic': topic_result['labels'][0],\n        'topic_scores': topic_result['scores'],\n        'topic_labels': topic_result['labels'],\n        'style': primary_style,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.322916Z","iopub.execute_input":"2024-11-20T20:54:21.323156Z","iopub.status.idle":"2024-11-20T20:54:21.337299Z","shell.execute_reply.started":"2024-11-20T20:54:21.323133Z","shell.execute_reply":"2024-11-20T20:54:21.336592Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Prompt Enhancement Function**","metadata":{}},{"cell_type":"code","source":"def generate_enhanced_prompt(original_prompt, analysis):\n    # Quality and style enhancement modifiers\n    quality_modifiers = [\n        \"highly detailed\",\n        \"sharp focus\",\n        \"professional photography\",\n        \"8k uhd\",\n        \"golden ratio composition\",\n        \"best quality\",\n        \"masterpiece\"\n    ]\n    \n    style_modifiers = {\n        'realistic': [\n            \"photorealistic\",\n            \"hyperrealistic\",\n            \"photograph\",\n            \"documentary style\",\n            \"natural lighting\"\n        ],\n        'artistic': [\n            \"digital art\",\n            \"concept art\",\n            \"trending on artstation\",\n            \"professional artwork\"\n        ],\n        'fantasy': [\n            \"ethereal\",\n            \"mystical atmosphere\",\n            \"fantasy art\",\n            \"magical environment\"\n        ],\n        'sci-fi': [\n            \"futuristic\",\n            \"cinematic lighting\",\n            \"sci-fi atmosphere\",\n            \"technological\"\n        ]\n    }\n\n    # Select style-specific modifiers\n    chosen_style_modifiers = style_modifiers.get(analysis['style'], [\"photorealistic\"])\n    subject_emphasis = (\n        f\"a clear detailed view of {original_prompt}, \"\n        f\"subject centered and in focus, \"\n        f\"main subject prominently featured, \"\n        f\"perfect composition\"\n    )\n    \n    # Combine modifiers to create an enhanced prompt\n    enhanced_prompt = (\n        f\"{subject_emphasis}, \"\n        f\"{random.choice(chosen_style_modifiers)}, \"\n        f\"{random.choice(quality_modifiers)}, \"\n        f\"professional lighting, intricate details\"\n    )\n    \n    return enhanced_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.338375Z","iopub.execute_input":"2024-11-20T20:54:21.338730Z","iopub.status.idle":"2024-11-20T20:54:21.356767Z","shell.execute_reply.started":"2024-11-20T20:54:21.338693Z","shell.execute_reply":"2024-11-20T20:54:21.356078Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Image Generation Function**","metadata":{}},{"cell_type":"code","source":"def generate_images(model, prompt, device, num_images=3, height=512, width=512, guidance_scale=7.5):\n    # Negative prompt to avoid undesired image characteristics\n    negative_prompt = (\n        \"blurry, low quality, low resolution, watermark, text, logo, signature, \"\n        \"deformed, disfigured, mutation, extra limbs, wrong composition, \"\n        \"bad anatomy, wrong anatomy, duplicate, multiple, cross-eye, \"\n        \"out of frame, ugly, extra digit, fewer digits, cropped, jpeg artifacts, \"\n        \"worst quality, poorly drawn, distorted, writing, letters\"\n    )\n    \n    images = []\n    \n    for i in range(num_images):\n        # Set random seed for reproducibility\n        seed = random.randint(0, 1_000_000)\n        generator = torch.Generator(device).manual_seed(seed)\n        \n        try:\n            # Generate image using Stable Diffusion\n            image = model(\n                prompt,\n                height=height,\n                width=width,\n                num_inference_steps=30,\n                guidance_scale=guidance_scale,\n                negative_prompt=negative_prompt,\n                generator=generator\n            ).images[0]\n            images.append(image)\n        except Exception as e:\n            print(f\"Error generating image {i+1}: {e}\")\n            continue\n            \n    return images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.357654Z","iopub.execute_input":"2024-11-20T20:54:21.357936Z","iopub.status.idle":"2024-11-20T20:54:21.375216Z","shell.execute_reply.started":"2024-11-20T20:54:21.357912Z","shell.execute_reply":"2024-11-20T20:54:21.374337Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Main Processing Function**","metadata":{}},{"cell_type":"code","source":"def process_and_generate(prompt, num_images=3, resolution=512, guidance_scale=7.5):\n    # Check if models are initialized\n    if not all([SENTIMENT_ANALYZER, TOPIC_CLASSIFIER, MODEL]):\n        return None, None, None, \"Error: Models not initialized.\"\n    \n    try:\n        # Clean and validate prompt\n        cleaned_prompt = prompt.strip()\n        if not cleaned_prompt:\n            return None, None, None, \"Error: Please enter a prompt.\"\n        \n        # Analyze and enhance prompt\n        analysis = analyze_prompt(cleaned_prompt, SENTIMENT_ANALYZER, TOPIC_CLASSIFIER)\n        enhanced_prompt = generate_enhanced_prompt(cleaned_prompt, analysis)\n        \n        # Generate images\n        images = generate_images(\n            MODEL, \n            enhanced_prompt, \n            DEVICE, \n            num_images=num_images, \n            height=resolution, \n            width=resolution,\n            guidance_scale=guidance_scale\n        )\n        \n        # Handle image generation failure\n        if not images:\n            return None, None, None, \"Error: Failed to generate images. Please try again.\"\n        \n        # Create visualization charts\n        sentiment_chart = create_sentiment_pie_chart(\n            analysis['sentiment_score'], \n            analysis['sentiment']\n        )\n        \n        topic_chart = create_topic_bar_chart(\n            topic_scores=analysis['topic_scores'], \n            topic_labels=analysis['topic_labels']\n        )\n        \n        # Prepare detailed information\n        details = (\n            f\"  Original Prompt:   {cleaned_prompt}\\n\\n\"\n            f\"  Enhanced Prompt:   {enhanced_prompt}\\n\\n\"\n            f\"  Style:             {analysis['style']}\\n\\n\"\n            f\"  Primary Topic:     {analysis['primary_topic']}\\n\\n\"\n            f\"  Sentiment:         {analysis['sentiment']} (Score: {analysis['sentiment_score']:.2f})\"\n        )\n        \n        return images, sentiment_chart, topic_chart, details\n    \n    except Exception as e:\n        return None, None, None, f\"Error during generation: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.376622Z","iopub.execute_input":"2024-11-20T20:54:21.376988Z","iopub.status.idle":"2024-11-20T20:54:21.392927Z","shell.execute_reply.started":"2024-11-20T20:54:21.376950Z","shell.execute_reply":"2024-11-20T20:54:21.391701Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Gradio Interface Setup**","metadata":{}},{"cell_type":"code","source":"# Global model initialization\nDEVICE = setup_environment()\nSENTIMENT_ANALYZER, TOPIC_CLASSIFIER, MODEL = initialize_models(DEVICE)\n\n# Create Gradio interface\ninterface = gr.Blocks(theme='default')\n\nwith interface:\n    # Markdown headers and description\n    gr.Markdown(\"# Emotion-Driven Visuals: Generating Art and Phrases from Text\")\n    gr.Markdown(\n        \"A Creative AI System for Emotion-Based Image and Phrase Generation.\\n\\n\"\n        \"Generate high-quality images from your text descriptions.\\n\"\n        \"The system will analyze the sentiment of your input and enhance your prompt while maintaining your original subject matter.\"\n    )\n    \n    # Input components\n    with gr.Row():\n        with gr.Column():\n            prompt_input = gr.Textbox(\n                label=\"Enter your prompt\", \n                placeholder=\"Describe what you want to generate...\", \n                lines=3\n            )\n            \n            # Sliders and dropdowns for customization\n            with gr.Row():\n                num_images_slider = gr.Slider(\n                    minimum=1, maximum=5, value=3, step=1, \n                    label=\"Number of Images\"\n                )\n                resolution_dropdown = gr.Dropdown(\n                    [512, 768, 1024], \n                    value=512, \n                    label=\"Image Resolution\"\n                )\n                guidance_slider = gr.Slider(\n                    minimum=1, maximum=15, value=7.5, step=0.5, \n                    label=\"Creativity/Guidance Scale\"\n                )\n            \n            # Generate and clear buttons\n            with gr.Row():\n                generate_btn = gr.Button(\"Generate Images\", variant=\"primary\")\n                clear_btn = gr.Button(\"Clear Everything\", variant=\"secondary\")\n    \n    # Output components\n    with gr.Row():\n        gallery = gr.Gallery(label=\"Generated Images\", show_label=True, elem_id=\"gallery\")\n        \n        with gr.Column():\n            sentiment_plot = gr.Plot(label=\"Sentiment Analysis\")\n            topic_plot = gr.Plot(label=\"Topic Classification\")\n    \n    details_box = gr.Textbox(label=\"Generation Details\", lines=6)\n    \n    # Button click event handlers\n    generate_btn.click(\n        fn=process_and_generate, \n        inputs=[prompt_input, num_images_slider, resolution_dropdown, guidance_slider],\n        outputs=[gallery, sentiment_plot, topic_plot, details_box]\n    )\n    \n    # Clear button functionality\n    clear_btn.click(\n        fn=lambda: (None, None, None, \"\", 3, 512, 7.5, \"\"),\n        inputs=None,\n        outputs=[\n            gallery, \n            sentiment_plot, \n            topic_plot, \n            details_box, \n            num_images_slider, \n            resolution_dropdown, \n            guidance_slider, \n            prompt_input\n        ]\n    )\n\n# Launch the Gradio interface\ninterface.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:54:21.394258Z","iopub.execute_input":"2024-11-20T20:54:21.394599Z","iopub.status.idle":"2024-11-20T20:55:10.536921Z","shell.execute_reply.started":"2024-11-20T20:54:21.394564Z","shell.execute_reply":"2024-11-20T20:55:10.536102Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, CUDA Available: True\nGPU: Tesla T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26bae94936a1472888bb83d8e216ffd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0077c9ecd7343ca977661f4fc7485bd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80febf311e81435483ca3af6668d1689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd6e6bc7446b457b8943e8a86d255354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3139b27150df441fb64d086f0e84f6fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51fd04aca686417895473cfaf472642f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6f70c8ce634f439fa2f4657ae626f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac8aa0a95f14ffaa945f4d7c439aa0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5dd363e93c04019a18a102823c985ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34edb628d624fe28c2fce36141ee9d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82cbde8591874f3c88e239c900c64276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6c2da17e024e9a8deaecdd89f92be1"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f6d3e283984aa28fe2e881b0f48fa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e68b41522071487ab0261bc8b3e03646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(â€¦)ature_extractor/preprocessor_config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787ab7d540944d67925dd807d9793c4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd811193ec1249f3ab05eb5e7b412569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a580ece9b7444fbcb04e94c6446746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3481fa2daa504df6a4073da244457b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3956b39700df4022b4dae02804da4b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a12f3a54bb419ea952edd7b060bb87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e89ee9ebbf460da9c755fd01b8817b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78471dc5d5fe4e91856680f5b843a407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8281844ce3426bbe035812c6223644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/756 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08112ad298448f4a4c51dfae49adcc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65483fb173a64cb881d6757a331b8ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c70a9be53544d72b439bc2e5ba27624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90d2093162e46e2b8996900019f6e0b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n","output_type":"stream"},{"name":"stdout","text":"Models Initialized Successfully\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://27e72db1ae9d0c98d3.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://27e72db1ae9d0c98d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}